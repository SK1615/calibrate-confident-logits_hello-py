# Calibrate Those Confident Logits â€“ Reinforcement Learning Task

## ðŸŽ¯ Objective
This RL task teaches a practical ML engineering skill â€” **probability calibration**.  
Given over-confident classification logits, the goal is to learn a calibrator on **validation data** and apply it to **test data**, returning valid probabilities.  
The task models a realistic ML workflow where models must remain reliable under mild distribution shift.

---

## ðŸ§  Task Description
You are provided with:
- `logits_val [Nv, K]` â€“ validation logits  
- `y_val [Nv]` â€“ validation labels  
- `logits_test [Nt, K]` â€“ test logits  

Implement:
```python
def solve(logits_val: np.ndarray, y_val: np.ndarray, logits_test: np.ndarray) -> np.ndarray:
    """
    Fit a calibrator ONLY on (logits_val, y_val) and apply it to logits_test.
    Return calibrated probabilities [Nt, K], non-negative, with each row summing to 1.
    Any correct calibration method is allowed (temperature scaling, isotonic, Platt,
    histogram binning, Dirichlet, vector/bias scaling, etc.).
    Use only numpy; keep it deterministic; no file or network access.
    """

Grading (on TEST) :
Metric	Requirement	Description
NLL improvement	â‰¥ REQ_NLL_IMP	Negative log-likelihood must improve
ECE improvement	â‰¥ REQ_ECE_IMP	Expected calibration error must improve
Mean max-confidence reduction	â‰¥ REQ_CONF_RED	Reduce over-confidence
Accuracy drop	â‰¤ MAX_ACC_DROP	Maintain classification accuracy

Thresholds are tuned for a 10â€“40% pass rate across random seeds.

Run Locally:
python grade.py              # single-seed smoke run (may PASS/FAIL)
python run_many.py --n 20    # shows pass-rate; aim for 10â€“40%

Tune Thresholds (optional):
python tune_thresholds.py --n 20


Run with LLM (Anthropic Claude) :

Set your Anthropic API key first:

Mac/Linux:
export ANTHROPIC_API_KEY="sk-ant-******"

Windows PowerShell:
setx ANTHROPIC_API_KEY "sk-ant-******"

Then run:
python llm_attempt.py --model claude-3-5-haiku-latest --n 10

This writes the LLMâ€™s generated solution to solve_backup.py and evaluates it via the stable wrapper solve.py.

Tests :

Run all tests to confirm environment integrity:
pytest -v

Expected output :

test_grade_smoke.py::test_grade_smoke :PASSED
test_metrics_and_data.py::test_make_synthetic_shapes :PASSED
test_metrics_and_data.py::test_softmax_valid :PASSED
test_metrics_and_data.py::test_metrics_monotonicity :PASSED
test_wrapper_contract.py::test_solve_contract :PASSED